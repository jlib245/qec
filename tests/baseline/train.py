import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from qec_sim.data.dataset import OfflineQECDataset
from qec_sim.models.baseline import ErasureAwareMLP

def train_model():
    # 1. í•˜ë“œì›¨ì–´ ì„¤ì • (GPUê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ë§¥ë¶ì´ë©´ MPS, ì—†ìœ¼ë©´ CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else 
                          "mps" if torch.backends.mps.is_available() else "cpu")
    print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

    # 2. ë°ì´í„° ë¡œë“œ
    print("\në°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    train_dataset = OfflineQECDataset("datasets/d5_complex_noise/train.npz")
    val_dataset = OfflineQECDataset("datasets/d5_complex_noise/val.npz")

    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)

    # ë°ì´í„°ì˜ í˜•íƒœë¥¼ í™•ì¸í•˜ì—¬ ëª¨ë¸ ì…ë ¥ í¬ê¸° ìë™ ì„¤ì •
    sample_x, sample_y = train_dataset[0]
    num_detectors = sample_x.shape[1]    # xì˜ í˜•íƒœ: (2ì±„ë„, num_detectors)
    num_observables = sample_y.shape[0]  # yì˜ í˜•íƒœ: (num_observables,)

    print(f"ë””í…í„° ìˆ˜: {num_detectors}, ë…¼ë¦¬ íë¹„íŠ¸ ìˆ˜: {num_observables}")

    # 3. ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
    model = ErasureAwareMLP(num_detectors, num_observables).to(device)
    
    # BCEWithLogitsLoss: ëª¨ë¸ì˜ ì¶œë ¥(Logits)ì— Sigmoidë¥¼ ì”Œìš°ê³  ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 4. í•™ìŠµ ë£¨í”„ (Train Loop)
    epochs = 20
    print("\nğŸš€ ë³¸ê²©ì ì¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()           # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
            outputs = model(batch_x)        # ìˆœì „íŒŒ (Forward)
            
            loss = criterion(outputs, batch_y) # ì†ì‹¤ ê³„ì‚°
            loss.backward()                 # ì—­ì „íŒŒ (Backward)
            optimizer.step()                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
            
            train_loss += loss.item() * batch_x.size(0)
            
        train_loss /= len(train_dataset)

        # 5. ê²€ì¦ ë£¨í”„ (Validation Loop)
        model.eval()
        val_loss = 0.0
        correct_predictions = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                
                loss = criterion(outputs, batch_y)
                val_loss += loss.item() * batch_x.size(0)
                
                # ì˜ˆì¸¡ê°’ ë„ì¶œ (0.5 ì´ìƒì´ë©´ 1, ì•„ë‹ˆë©´ 0)
                predictions = (torch.sigmoid(outputs) > 0.5).float()
                
                # ë°°ì¹˜ ë‚´ì—ì„œ ë…¼ë¦¬ì  ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šì€(ì™„ë²½íˆ ë§ì¶˜) ê°œìˆ˜ ê³„ì‚°
                # (ëª¨ë“  observableì„ ë§ì·„ì„ ë•Œ ì •ë‹µìœ¼ë¡œ ì¸ì •)
                correct_predictions += (predictions == batch_y).all(dim=1).sum().item()
                
        val_loss /= len(val_dataset)
        logical_error_rate = 1.0 - (correct_predictions / len(val_dataset))

        # ê²°ê³¼ ì¶œë ¥
        print(f"[Epoch {epoch+1:02d}/{epochs}] "
              f"Train Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"Val Logical Error Rate: {logical_error_rate * 100:.2f}%")

if __name__ == "__main__":
    train_model()